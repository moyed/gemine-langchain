{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOKOSZcWV94HZWe0mkGWTlk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/moyed/gemine-langchain/blob/main/01_hello_gemini.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5LmM7FLGNZRO",
        "outputId": "edcf8a3b-6881-4f28-d450-11967c1bef15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: LangChain is a framework designed to simplify the development of applications powered by large language models (LLMs). Think of it as a toolkit that provides building blocks and abstractions to help you:\n",
            "\n",
            "**Core Concepts:**\n",
            "\n",
            "* **Chains:** These are sequences of calls to LLMs or other utilities. They allow you to create complex workflows by linking together different operations, such as prompting, data retrieval, and output parsing. For example, you could create a chain that first retrieves relevant information from a database, then uses an LLM to generate a summary based on that information.\n",
            "* **Components:** LangChain provides a wide range of reusable components, including:\n",
            "    * **LLMs:** Wrappers for various LLMs (e.g., OpenAI, Hugging Face, Google).\n",
            "    * **Prompts:** Tools for creating and managing prompts for LLMs.\n",
            "    * **Indexes:** Tools for indexing and retrieving information from various sources.\n",
            "    * **Memory:** Mechanisms for storing and retrieving conversational history.\n",
            "    * **Agents:** Systems that can use LLMs to decide which tools to use based on a given task.\n",
            "    * **Callbacks:** Mechanisms for tracking the execution of chains and components.\n",
            "    * **Output Parsers:** Tools for formatting the output of LLMs into structured data.\n",
            "\n",
            "**Key Benefits of Using LangChain:**\n",
            "\n",
            "* **Abstraction:** It abstracts away the complexities of working directly with LLMs, allowing developers to focus on building application logic rather than managing API calls and prompt engineering.\n",
            "* **Modularity:** Components are designed to be reusable and composable, making it easy to build complex applications by combining simpler elements.\n",
            "* **Flexibility:** Supports a wide variety of LLMs, data sources, and use cases.\n",
            "* **Rapid Prototyping:** Simplifies the process of experimenting with different LLM configurations and workflows.\n",
            "* **Community Support:** Active community providing support, tutorials, and examples.\n",
            "\n",
            "**In essence, LangChain makes it easier to:**\n",
            "\n",
            "* **Build conversational AI applications:** Create chatbots, virtual assistants, and other interactive experiences.\n",
            "* **Automate tasks:** Develop applications that can perform tasks such as data analysis, content generation, and code completion.\n",
            "* **Integrate LLMs with other systems:** Connect LLMs to databases, APIs, and other data sources.\n",
            "* **Experiment with different LLM strategies:** Easily test and compare different prompting techniques and workflows.\n",
            "\n",
            "**Think of it like this:**\n",
            "\n",
            "Imagine you're building a house. You could try to make every single brick and nail yourself, but that would be inefficient. LangChain provides you with the pre-made bricks, windows, and doors (the components) and a blueprint (the chains) to make building your house (the LLM application) much easier and faster.\n",
            "\n",
            "**In summary, LangChain is a powerful framework that significantly simplifies the development of LLM-powered applications by providing reusable components, abstractions, and a structured approach to building complex workflows.** It's a valuable tool for anyone looking to leverage the power of LLMs in their projects.\n",
            "\n",
            "Answer: Okay, let's break down the difference between vertical and horizontal scaling, especially in a way that makes sense for a fresh graduate. Think of it like managing a restaurant:\n",
            "\n",
            "**The Core Problem: You need to serve more customers (handle more traffic/data)**\n",
            "\n",
            "Imagine you have a small, successful restaurant. You're getting more and more customers, and your current setup can't handle the demand. You need to scale your operations. Here are the two main ways you can do it:\n",
            "\n",
            "**1. Vertical Scaling (Scaling Up): Getting a Bigger Oven**\n",
            "\n",
            "* **Concept:**  Vertical scaling means **increasing the resources of a single machine**. It's like upgrading your existing oven to a bigger, more powerful one.\n",
            "* **How it works in the tech world:** You'd add more CPU (processing power), RAM (memory), or storage to your existing server.\n",
            "* **Analogy:** In our restaurant, you'd get a larger oven that can cook more food at once. You'd also upgrade your existing kitchen appliances to be more powerful.\n",
            "* **Pros:**\n",
            "    * **Simpler to manage:** You're still dealing with one server.\n",
            "    * **Often easier to implement:** Upgrading hardware is usually straightforward.\n",
            "    * **Can be cost-effective initially:** For smaller increases in demand, it might be cheaper than adding a whole new server.\n",
            "* **Cons:**\n",
            "    * **Limited by hardware:** You can only upgrade a single machine so much. Eventually, you'll hit a limit.\n",
            "    * **Single point of failure:** If that one server goes down, your entire system goes down.\n",
            "    * **Can be expensive at scale:** Top-of-the-line hardware can get very pricey.\n",
            "\n",
            "**2. Horizontal Scaling (Scaling Out): Opening Another Restaurant**\n",
            "\n",
            "* **Concept:** Horizontal scaling means **adding more machines to your system**. It's like opening a second (or third, fourth...) restaurant.\n",
            "* **How it works in the tech world:** You would add more servers to your system, all working together.\n",
            "* **Analogy:** In our restaurant example, you'd open a new branch of your restaurant. Now you have two kitchens and two dining areas, able to serve twice as many customers.\n",
            "* **Pros:**\n",
            "    * **Highly scalable:** You can add servers indefinitely to handle increasing demand.\n",
            "    * **More fault-tolerant:** If one server goes down, the others can still handle the load.\n",
            "    * **Can be more cost-effective at scale:** Adding multiple smaller servers can be cheaper than buying one massive server.\n",
            "* **Cons:**\n",
            "    * **More complex to manage:** You need to manage multiple servers.\n",
            "    * **Requires load balancing:** You need a way to distribute traffic evenly across the servers.\n",
            "    * **Can be more complex to implement:** Setting up distributed systems takes more planning.\n",
            "\n",
            "**Here's a table summarizing the key differences:**\n",
            "\n",
            "| Feature        | Vertical Scaling (Scaling Up)        | Horizontal Scaling (Scaling Out)      |\n",
            "|----------------|---------------------------------------|---------------------------------------|\n",
            "| **Method**      | Increase resources of a single machine | Add more machines to the system      |\n",
            "| **Analogy**     | Bigger oven                           | Opening another restaurant           |\n",
            "| **Complexity**  | Simpler to manage                    | More complex to manage                |\n",
            "| **Scalability** | Limited by hardware                   | Highly scalable                      |\n",
            "| **Fault Tolerance** | Single point of failure              | More fault-tolerant                   |\n",
            "| **Cost**        | Can be cost-effective initially      | Can be more cost-effective at scale |\n",
            "\n",
            "**Which one is better?**\n",
            "\n",
            "There's no single \"better\" option. The choice depends on the specific needs of your application:\n",
            "\n",
            "* **Vertical scaling** is often a good starting point for smaller applications or when you need to quickly boost performance.\n",
            "* **Horizontal scaling** is essential for large, high-traffic applications that need to be reliable and scalable.\n",
            "\n",
            "**Key takeaway for a Fresh Graduate:**\n",
            "\n",
            "As a fresh grad, understanding these concepts is crucial. You'll likely encounter both approaches in your career.  It's important to know:\n",
            "\n",
            "* **Vertical scaling is simpler but has limitations.**\n",
            "* **Horizontal scaling is more complex but provides greater scalability and reliability.**\n",
            "* **Most modern applications often utilize a combination of both techniques.**\n",
            "\n",
            "Don't worry if you don't fully grasp all the intricacies right away. The more you work with systems and infrastructure, the more intuitive these concepts will become.  Focus on understanding the core ideas and you'll be well on your way!\n",
            "\n",
            "Good luck with your learning journey! Let me know if you have any more questions.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Install necessary package for LangChain integration with Google GenAI\n",
        "!pip install -U -q langchain-google-genai\n",
        "\n",
        "# Import required modules\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "# Fetch the Google API Key securely from user-provided data in Google Colab\n",
        "from google.colab import userdata\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "# Initialize the Google GenAI Large Language Model\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    api_key=GOOGLE_API_KEY,\n",
        "    model=\"gemini-2.0-flash-exp\",\n",
        "    temperature=0.7\n",
        ")\n",
        "\n",
        "# Define a prompt template for general questions\n",
        "# This will guide the model's behavior and format its response\n",
        "prompt_template = PromptTemplate(\n",
        "    input_variables=[\"question\"],\n",
        "    template=\"You are a helpful assistant. Answer the following question:\\n\\n{question}\"\n",
        ")\n",
        "\n",
        "# This simplifies interaction with the LLM\n",
        "chain = LLMChain(llm=llm, prompt=prompt_template)\n",
        "\n",
        "# Define the question for the first interaction\n",
        "question = \"What is LangChain?\"\n",
        "\n",
        "# Output the response to the user\n",
        "response = chain.run({\"question\": question})\n",
        "print(\"Answer:\", response)\n",
        "\n",
        "# Define a second prompt template tailored for mentoring fresh graduates\n",
        "prompt_template = PromptTemplate(\n",
        "    input_variables=[\"question\"],\n",
        "    template=\"You are a helpful mentor. Answer the following question:\\n\\n{question} for fresh graudate\"\n",
        ")\n",
        "# Create another chain using the mentor-specific prompt\n",
        "chain = LLMChain(llm=llm, prompt=prompt_template)\n",
        "\n",
        "# Define the second question\n",
        "question = \"What is the difference between vertical and horizontal scaling?\"\n",
        "\n",
        "# Generate the response for the second question\n",
        "response = chain.run({\"question\": question})\n",
        "print(\"Answer:\", response)\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}